# NLP bootcamp
모두의연구소 flipped school 과정 중 하나인 NLP bootcamp에서 발표한 paper 목록과 그 자료들입니다.

+ participant : 강재욱, 권성은, 김경환, 김동화, 김민섭, 김수정, 김승일,  김충현, 모경현, 박정배, 박희경, 성나영, 염혜원, 원종국, 윤훈상, 이승재, 이일구, 이현준, 정미연, 최우정, 조주현
+ faciliator : 김보섭

## Schedule
### Week01
Orientation
### Week02
+ Convolutional Neural Networks for Sentence Classification
  - Presenter : 최우정 
  - Paper : https://arxiv.org/abs/1408.5882
  - Material : [Convolutional Neural Networks for Sentence Classification_최우정.pdf](https://github.com/modulabs/NLP-bootcamp/blob/master/week02/Convolutional%20Neural%20Networks%20for%20Sentence%20Classification_%EC%B5%9C%EC%9A%B0%EC%A0%95.pdf)
+ Character-level Convolutional Networks for Text Classification
  - Presenter : 박정배
  - Paper : https://arxiv.org/abs/1509.01626
  - Material :  [Character-level convolutional networks for text classification_박정배.pdf](https://github.com/modulabs/NLP-bootcamp/blob/master/week02/Character-level%20convolutional%20networks%20for%20text%20classification_%EB%B0%95%EC%A0%95%EB%B0%B0.pdf)
### Week03
+ Character-Aware Neural Language Models
  - Presenter : 이일구
  - Paper : https://arxiv.org/abs/1508.06615
  - Material : 
+ A Convolutional Neural Network for Modelling Sentences
  - Presenter : 윤훈상
  - Paper : https://arxiv.org/abs/1404.2188
  - Material : [A Convolutional Neural Network for Modelling Sentences_윤훈상.pdf](https://github.com/modulabs/NLP-bootcamp/blob/master/week03/A%20Convolutional%20Neural%20Network%20for%20Modelling%20Sentences_%EC%9C%A4%ED%9B%88%EC%83%81.pdf)
### Week04
+ Learning Phrase Representation using RNN Encoder-Decoder for Statistical Machine Translation
	- Presenter : 염혜원
	- Paper : https://arxiv.org/abs/1406.1078
	- Material : [Learning Phrase Representation using RNN Encoder-Decoder for Statistical Machine Translation_염혜정.pdf](https://github.com/modulabs/NLP-bootcamp/blob/master/week04/Learning%20Phrase%20Representation%20using%20RNN%20Encoder-Decoder%20for%20Statistical%20Machine%20Translation_%EC%97%BC%ED%98%9C%EC%A0%95.pdf) 
+ Sequence to Sequence Learning with Neural Networks
	- Presenter : 권성은
	- Paper : https://arxiv.org/abs/1409.3215
	- Material :  [Sequence to Sequence Learning with Neural Networks_권성은.pdf](https://github.com/modulabs/NLP-bootcamp/blob/master/week04/Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks_%EA%B6%8C%EC%84%B1%EC%9D%80.pdf)
### Week05
+ A Neural Conversational Model
	- Presenter : 원종국
	- Paper : https://arxiv.org/abs/1506.05869
	- Material :
+ Convolutional Sequence to Sequence Learning
	- Presenter : 모경현
	- Paper : https://arxiv.org/abs/1705.03122
	- Material :   
### Week06
+ Neural Machine Translation by Jointly Learning to Align and Translate
	- Presenter : 박희경
	- Paper : https://arxiv.org/abs/1409.0473
	- Material : 
+ Effective Approaches to Attention-based Neural Machine Translation
	- Presenter : 김보섭
	- Paper : https://arxiv.org/abs/1508.04025
	- Material
### Week07
+ A Decomposable Attention Model for Natural Language Inference
	- Presenter : 정미연
	- Paper : https://arxiv.org/abs/1606.01933
	- Material : 
+ Attention is All You Need
	- Presenter : 이승재
	- Paper : https://arxiv.org/abs/1706.03762
	- Material :  
### Week08
+ Show and Tell: A Neural Image Caption Generator
	- Presenter : 김경환
	- Paper : https://arxiv.org/abs/1411.4555
	- Material : 
+ Show, Attend and Tell: Neural Image Caption Generation with Visual Attention
	- Presenter : 이현준
	- Paper : https://arxiv.org/abs/1502.03044
	- Material : 
### Week09
+ Memory Networks
	- Presenter : 김충현
	- Paper : https://arxiv.org/abs/1410.3916
	- Material : 
+ End-To-End Memory Networks
	- Presenter : 조주현
	- Paper : https://arxiv.org/abs/1503.08895
	- Material :   
### Week10
+ Ask Me Anything: Dynamic Memory Networks for Natural Language Processing
	- Presenter : 김승일
	- Paper : https://arxiv.org/abs/1506.07285
	- Material : 
+ Enriching Word Vectors with Subword Information
	- Presenter : 성나영
	- Paper : https://arxiv.org/abs/1607.04606
	- Material : 
### Week11
졸업식
### Week12
+ Deep contextualized word representations
	- Presenter : 김민섭
	- Paper : https://arxiv.org/abs/1802.05365
	- Material : 
+ BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
	- Presenter : 김동화
	- Paper : https://arxiv.org/abs/1810.04805
	- Material :  
